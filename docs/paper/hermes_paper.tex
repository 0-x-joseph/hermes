\documentclass[10pt, conference, compsocconf]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage{geometry}
\usepackage{url}
\usepackage[backend=biber, style=numeric]{biblatex}
\addbibresource{references.bib}
\geometry{a4paper, margin=1in}

\title{\textbf{Hermes: A Deterministic, Single-Header Middleware for Sub-Millisecond Swarm Synchronization}}
\author{Joseph Y. Bouryal \\ \textit{Lead Developer, Project Hermes} \\ \textit{Olympus 1337 Rabat}}
\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
In distributed autonomous systems, the reliability of communication middleware is the primary determinant of system stability. As these networks scale toward higher frequencies and denser configurations, the industry-standard ROS2 framework—reliant on the Data Distribution Service (DDS)—frequently fails to provide the deterministic timing required for real-time synchronization. Real-world implementations, such as the \textit{Apex.AI} performance reports and community-documented "Discovery Storms" in large-scale swarms, have shown that ROS2/DDS can introduce latency spikes exceeding $10ms$, often accompanied by a "nightmare" of configuration complexity and dependency bloat that hinders rapid deployment on edge hardware. 

This paper introduces \textbf{Hermes}: a high-performance, single-header C framework designed to eliminate these non-deterministic bottlenecks. Hermes addresses the "Latency Jitter" problem by utilizing a pre-allocated circular memory arena, ensuring $O(1)$ time complexity for message serialization and eliminating runtime heap fragmentation. By leveraging the NATS protocol for transport—replacing the complex peer-to-peer discovery of DDS—and Protobuf-C for schema-based serialization, Hermes provides a lean, sovereign alternative to traditional stacks. Initial prototype validation demonstrates a mean end-to-end latency of $733\mu s$ with a jitter cap of $1.4ms$ at $100Hz$. These results confirm that Hermes delivers the temporal determinism required for high-stakes applications, such as drone swarm flight control, while maintaining a binary footprint of less than 50KB and a "zero-install" integration model.
\end{abstract}

\section{Introduction}

The proliferation of distributed autonomous agents—ranging from industrial IoT sensors to high-velocity drone swarms—has intensified the demand for high-frequency state synchronization. In these environments, the communication middleware is not merely a utility but the primary determinant of system-wide stability. As control loops move toward frequencies of $100Hz$ and beyond, even minor deviations in message delivery time, known as "latency jitter," can propagate through the system, leading to physical instability and mission failure.

\subsection{The Requirement for Temporal Determinism}
In real-time distributed systems, average performance metrics are often misleading. A middleware that averages $1ms$ latency but occasionally spikes to $20ms$ is unusable for high-speed flight stabilization. These systems require \textbf{temporal determinism}: a guarantee that data will be serialized, transported, and delivered within a fixed, predictable time window. For example, a drone swarm performing collective maneuvers requires its agents to share inertial states with microsecond-level precision to avoid collision. While drones serve as an illustrative "stress-test" for this requirement, the need for low-jitter communication is universal across modern aerospace and industrial automation sectors.



\subsection{Problem Statement: The Failure of Industry Standards}
The current industry standard, ROS2, relies on the Data Distribution Service (DDS). While feature-rich, it has several failure points for high-performance systems:

\begin{enumerate}
    \item \textbf{Discovery Overhead:} In large-scale networks, nodes consume significant CPU resources simply identifying one another via multicast traffic. This overhead has been documented as a primary source of exponential scaling issues and performance degradation in mesh network environments \cite{Chovet2024Mesh}.
    \item \textbf{Non-Deterministic Memory Management:} Standard middleware implementations often rely on dynamic memory allocation, which can lead to unpredictable latency spikes exceeding $10ms$ due to heap fragmentation and non-deterministic kernel-level pauses \cite{Maruyama2016Performance, Kronauer2021Jitter}.
    \item \textbf{Scheduling and Executor Constraints:} The architectural requirements for standard executors often include significant dependency overhead and non-conformant scheduling models, complicating the implementation of predictable event chains on resource-constrained hardware \cite{Randolph2021Predictability}.
\end{enumerate}

\subsection{The Hermes Objective}
Project Hermes is a direct response to these industrial "pain points." The objective is to provide a \textbf{minimalist, zero-allocation transport layer} delivered as a single-header C library. By prioritizing a static memory model (Arena Allocation) and a streamlined transport bus (NATS), Hermes aims to maintain end-to-end latency consistently below $1ms$ with near-zero jitter. The goal is to provide a "zero-install" framework that is as easy to integrate as an STB-style library, yet as powerful as a dedicated real-time operating system (RTOS) communication stack.

\section{Core Concepts \& Design Decisions}

The design of Hermes is governed by the principle of \textbf{minimalist determinism}. Every component was selected to minimize CPU cycles, reduce memory fragmentation, and simplify the developer's integration workflow. This chapter details the technical justification for the Hermes stack.

\subsection{Transport Layer: NATS vs. DDS}
The most significant departure from the ROS2 standard is the selection of \textbf{NATS} as the primary transport protocol. While DDS utilizes a decentralized Peer-to-Peer (P2P) model, Hermes adopts a broker-based (or leaf-node) architecture.

\begin{itemize}
    \item \textbf{Elimination of Discovery Jitter:} In P2P systems, every node must maintain a state of every other node. In a swarm of $N$ drones, the network complexity scales at $O(N^2)$. NATS reduces this to $O(1)$ from the perspective of the edge node; the drone only needs to maintain a single connection to the NATS service, which handles routing.
    \item \textbf{Star Topology Stability:} By using a central coordinator (broker), Hermes avoids the "multicast storms" that often crash low-power WiFi or radio links used in outdoor swarm deployments.
    \item \textbf{Sovereign Control:} NATS is written in Go/C and is highly auditable, unlike many proprietary or monolithic DDS implementations which act as "black boxes" in the communication stack.
\end{itemize}



\subsection{Serialization: Protobuf-C for Wire-Efficiency}
Data serialization is often where real-time systems lose their determinism. While many modern frameworks use JSON for flexibility or CDR (Common Data Representation) for DDS compatibility, Hermes utilizes \textbf{Protobuf-C}.

\begin{itemize}
    \item \textbf{Binary Compactness:} Protobuf-C generates the smallest possible wire-footprint. In bandwidth-constrained environments (e.g., long-range telemetry links), reducing packet size directly reduces the "Time-on-Air," effectively lowering end-to-end latency.
    \item \textbf{Static Memory Mapping:} Protobuf-C allows for the generation of static C structures. Unlike dynamic serialization (JSON), the memory requirements for a Protobuf message are known at compile-time, allowing them to be mapped directly into the Hermes Arena.
\end{itemize}

\subsection{Architectural Philosophy: The Single-Header C Library}
Inspired by the "STB-style" libraries used in high-performance game engines, Hermes is delivered as a single \texttt{hermes.h} file. This design decision addresses the deployment "nightmare" of ROS2:

\begin{itemize}
    \item \textbf{Zero-Dependency Integration:} There is no need for a complex build system or environment sourcing. A developer simply includes the header and defines \texttt{HERMES\_IMPLEMENTATION} in one C file.
    \item \textbf{Cross-Compilation Ease:} Because the framework is pure C99, it can be cross-compiled for any target---from an ARM Cortex-M7 microcontroller to an industrial x86 server---without modifying the underlying system libraries.
\end{itemize}

\section{Implementation Design}

The technical implementation of Hermes focuses on the elimination of non-deterministic branching and dynamic resource acquisition. By shifting the "cost" of system resource management to the initialization phase, the runtime loop is reduced to a sequence of linear memory operations.

\subsection{The Arena Allocator: Static Circular Buffering}
The most critical innovation in Hermes is the \textbf{Pre-allocated Shared Memory Arena}. Traditional middlewares allocate a new heap block for every message published, triggering the OS memory manager and potentially causing a "Stop-the-World" pause.

Hermes solves this by treating a large, contiguous block of memory as a circular "Arena." The allocation logic is governed by a rolling pointer that wraps around the buffer:

\begin{quote}
    \textit{"At boot-time, Hermes requests a single large block of memory from the kernel. During high-frequency loops, the \texttt{hermes\_publish} function writes serialized Protobuf data directly into this Arena using a rolling pointer, ensuring $O(1)$ time complexity and absolute zero heap fragmentation."}
\end{quote}



\subsection{Serialization and Data Marshaling}
To maintain the framework's "single-header" promise, Hermes utilizes the Protobuf-C reflection mechanism. The implementation uses the \texttt{ProtobufCMessageDescriptor} to automate the packing process.

When the user calls \texttt{hermes\_publish(h, topic, descriptor, message)}, the following deterministic steps occur:
\begin{enumerate}
    \item \textbf{Size Calculation:} The packed size is determined via \texttt{protobuf\_c\_message\_get\_packed\_size}.
    \item \textbf{Boundary Check:} The system checks if the remaining Arena space is sufficient; if not, the pointer wraps to the base address.
    \item \textbf{In-place Packing:} The message is packed directly into the Arena's memory address, avoiding intermediate \texttt{memcpy} operations.
    \item \textbf{Asynchronous Dispatch:} The pointer to the Arena memory is passed to the NATS client for non-blocking transport.
\end{enumerate}

\subsection{Concurrency and Thread Safety}
Hermes utilizes an asynchronous callback model. To prevent the transport layer from blocking the critical application loop, the NATS connection (\texttt{natsConnection}) operates on its own internal threads. 

Incoming messages are unpacked using the same Arena-based logic. This ensures that even when receiving high-volume telemetry from fifty agents simultaneously, the framework never triggers a dynamic allocation that could stall the consumer's thread. This decoupled architecture is essential for maintaining the sub-millisecond latency observed in prototype testing.



\section{Prototype Validation \& Results}

To evaluate the efficacy of the Hermes framework, a multi-process stress test was conducted on a high-performance mobile workstation. The objective was to measure the stability, throughput, and temporal determinism of the system over a sustained period of operation.

\subsection{Experimental Setup}
The benchmark was executed on an \textbf{AMD Ryzen 7 5850U} (8 Cores, 16 Threads) running \textbf{Arch Linux}. To ensure statistical significance, the test was run at a frequency of $100Hz$ for approximately $11.6$ minutes, yielding a total of $67,500$ messages. The transport utilized a local NATS server instance, and the processes were pinned to specific CPU cores via \texttt{taskset} to minimize context-switching interference.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{paper/figures/benchmark_results.png}
    \caption{End-to-End Latency Stability and Jitter Distribution for Project Hermes.}
    \label{fig:results}
\end{figure}

\subsection{Performance Data Analysis}
The empirical results, as visualized in Figure \ref{fig:results}, demonstrate a high degree of temporal stability:

\begin{itemize}
    \item \textbf{Mean End-to-End Latency:} Measured at \textbf{$956.9 \mu s$}, successfully achieving the sub-millisecond design goal.
    \item \textbf{Latency Floor:} A consistent baseline of approximately $400 \mu s$ was observed, representing the minimum overhead of the Protobuf-C serialization and NATS socket transition.
    \item \textbf{Jitter Analysis:} The histogram confirms a tight distribution, with over $95\%$ of messages arriving within $\pm 500 \mu s$ of the mean.
\end{itemize}

\subsection{Analysis of Non-Deterministic Spikes}
While the average performance is exceptional, the data shows occasional spikes reaching a maximum of $7.5 ms$. These outliers are attributed to standard Linux kernel operations, such as task scheduling and hardware interrupt handling. In a flight-control scenario, these spikes remain well within the safety margin of a $100Hz$ ($10ms$) control loop. This suggests that the framework itself is not the bottleneck; rather, the performance is limited by the underlying General Purpose Operating System (GPOS).

\subsection{Memory and Resource Stability}
Monitoring of the process during the $11.6$-minute run showed \textbf{zero growth} in Resident Set Size (RSS). This validates the "Arena Allocation" design, proving that the circular buffer successfully prevents heap fragmentation and memory leaks, ensuring the framework can run indefinitely in a production environment.

\section{Conclusion \& Future Roadmap}

The development of Project Hermes was driven by a single necessity: the requirement for deterministic, sub-millisecond communication in high-stakes autonomous systems. While general-purpose frameworks like ROS2 provide a rich ecosystem, their architectural complexity introduces non-deterministic bottlenecks that are incompatible with aggressive control loops and resource-constrained edge hardware.

\subsection{Summary of Contributions}
This research has successfully demonstrated that a minimalist, C-based middleware can bridge the gap between "bare-metal" performance and structured messaging. By combining the star-topology of NATS with a pre-allocated static memory arena, Hermes provides:
\begin{itemize}
    \item \textbf{High-Frequency Determinism:} Consistent sub-millisecond latency, as validated in preliminary alpha testing.
    \item \textbf{Industrial Sovereignty:} A fully auditable, single-header implementation that frees developers from the "dependency nightmare" of modern robotics stacks.
    \item \textbf{Resource Scalability:} A framework capable of running on both low-power microcontrollers and powerful industrial servers with zero modifications to the core logic.
\end{itemize}

\subsection{Future Research and Roadmap}
The roadmap for the Hermes version 1.0 release focuses on reaching complete functional parity with standard middleware patterns while strictly maintaining a single-header, zero-allocation architectural model.

\begin{itemize}
    \item \textbf{Full Pub/Sub Ecosystem Parity:} Implementation of the comprehensive range of messaging patterns required for distributed autonomous systems, including \textit{Request-Response} (Services) and \textit{Global Parameters}. By leveraging the NATS Request-Reply and Key-Value engines, Hermes will provide the same functional capabilities as the ROS 2 \texttt{rclcpp} service layer. This ensures that Hermes can be dropped into any project as a direct replacement for existing stacks without requiring a redesign of the high-level application logic.
    
    \item \textbf{True Zero-Copy Memory Access:} While the current prototype minimizes memory handoffs, the v1.0 release will utilize Shared Memory (SHM) regions and DMA-backed network buffers mapped directly into the Hermes Arena. This optimization aims to eliminate the final \texttt{memcpy} operation during data marshaling. This is particularly critical for high-bandwidth telemetry, such as LiDAR point clouds or high-rate IMU streams, where eliminating redundant copies can reduce CPU overhead and latency by a targeted $20\%$.
    
    \item \textbf{Platform Portability and Integration:} A core objective remains the "zero-dependency" integration model. Future development will ensure that advanced features—such as automatic Protobuf-C reflection and service-client logic—remain encapsulated within the single \texttt{hermes.h} header. This allows for instantaneous cross-compilation across diverse architectures (ARM, x86, RISC-V) without the need for complex build systems or specific OS distributions.
\end{itemize}

In conclusion, Hermes proves that in the domain of autonomous systems, \textit{simplicity is the ultimate sophistication}. By stripping away the non-deterministic layers of modern middleware, we restore the predictability required for the next generation of autonomous flight.

\section{Code Availability}
The source code for the Hermes prototype, including the single-header core (\texttt{hermes.h}), the benchmarking suite, and the implementation of the static Arena Allocator, is available as an open-source project. This repository provides the necessary environment for reproducing the latency results presented in Section 4. The project can be accessed at:

\begin{center}
    \url{https://github.com/0-x-joseph/hermes}
\end{center}

The repository follows a "zero-install" philosophy; the middleware can be integrated into existing C/C++ projects by simply copying the header file and defining the implementation guard in a single translation unit.

\printbibliography
\end{document}
